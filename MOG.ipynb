{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45dbb78e-ba97-48ee-87c4-eeca3c4371fc",
   "metadata": {},
   "source": [
    "This notebook uses the processed files for CAT indices to calculate thresholds for moderate or greater MOG turbulence.\n",
    "\n",
    "First, we select a mid-lat region for defining MOG.\n",
    "\n",
    "Then we calculate the threshold or thresholds using quantiles.\n",
    "\n",
    "Once we have defined a threshold, we can use that value to calculate the frequency of exceeding that threshold.\n",
    "\n",
    "Frequency above thresholds can be used to evaluate time series and trends over time,\n",
    "as well as spatial distributions of the strongest areas of turbulence\n",
    "\n",
    "Files with MOG frequency netcdf files are saved for using to make plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "893e55a6-d8c9-4bfd-ad91-d623137a451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob\n",
    "import intake\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"flox\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e2b4cf-89c1-49bd-bf42-5b830b940759",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.10/lib/python3.11/site-packages/distributed/diagnostics/nvml.py:14: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml\n",
      "/g/data/xp65/public/apps/med_conda/envs/analysis3-25.10/lib/python3.11/site-packages/distributed/diagnostics/nvml.py:14: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-a5cd063d-c674-11f0-975d-000003cefe80</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> distributed.LocalCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <button style=\"margin-bottom: 12px;\" data-commandlinker-command=\"dask:populate-and-launch-layout\" data-commandlinker-args='{\"url\": \"/proxy/8787/status\" }'>\n",
       "                Launch dashboard in JupyterLab\n",
       "            </button>\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">LocalCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">d0b7e765</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 1\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 5\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 18.00 GiB\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "    <td style=\"text-align: left;\"><strong>Status:</strong> running</td>\n",
       "    <td style=\"text-align: left;\"><strong>Using processes:</strong> True</td>\n",
       "</tr>\n",
       "\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-f9ca70f0-7587-4dd8-b6c5-94f7a8f9d3ca</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://127.0.0.1:33345\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0 \n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 0</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:36507\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 5\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/43681/status\" target=\"_blank\">/proxy/43681/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 18.00 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:42725\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/154942544.gadi-pbs/dask-scratch-space/worker-dehkl5kr\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:33345' processes=1 threads=5, memory=18.00 GiB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-217021' coro=<Client._gather.<locals>.wait() done, defined at /g/data/xp65/public/apps/med_conda/envs/analysis3-25.10/lib/python3.11/site-packages/distributed/client.py:2384> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/xp65/public/apps/med_conda/envs/analysis3-25.10/lib/python3.11/site-packages/distributed/client.py\", line 2393, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n"
     ]
    }
   ],
   "source": [
    "from plotting_maps.acs_plotting_maps import plot_acs_hazard_multi, plot_acs_hazard, plot_data, cmap_dict, regions_dict\n",
    "from matplotlib import colors, cm\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "# ProgressBar().register()\n",
    "\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "client = Client(threads_per_worker=5, n_workers=1)\n",
    "# client = Client(threads_per_worker=4, n_workers=7)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "057d170c-37e5-45ec-9fbc-3e94f9311306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Detected cluster: 1 workers × 5 threads each\n",
      "✅ Estimated concurrent chunks per worker: 5\n",
      "✅ Max safe chunk size: ~3150.00 MB\n",
      "✅ Suggested chunking: time=1460, lat=436, lon=618\n"
     ]
    }
   ],
   "source": [
    "def suggest_chunking(client, \n",
    "                     worker_memory_gb=126/4, \n",
    "                     reserve_fraction=0.5, \n",
    "                     dtype=\"float64\", \n",
    "                     time_steps_per_year=1460):\n",
    "    # Connect to active Dask client\n",
    "    cluster_info = client.scheduler_info()\n",
    "\n",
    "    # Detect number of workers and threads per worker\n",
    "    num_workers = len(cluster_info[\"workers\"])\n",
    "    threads_per_worker = list(cluster_info[\"workers\"].values())[0][\"nthreads\"]\n",
    "    concurrent_chunks_per_worker = threads_per_worker  # For I/O-bound tasks\n",
    "\n",
    "    # Calculate safe memory per worker\n",
    "    usable_mem_bytes = worker_memory_gb * (1 - reserve_fraction) * 1e9\n",
    "    max_chunk_bytes = usable_mem_bytes / concurrent_chunks_per_worker\n",
    "    max_chunk_mb = max_chunk_bytes / 1e6\n",
    "\n",
    "    # Determine bytes per value\n",
    "    bytes_per_value = 4 if dtype == \"float32\" else 8\n",
    "    max_elements = max_chunk_bytes / bytes_per_value\n",
    "    target_lat_lon_elements = max_elements / time_steps_per_year\n",
    "\n",
    "    suggested_lat = 436\n",
    "    suggested_lon = int(target_lat_lon_elements//436)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"✅ Detected cluster: {num_workers} workers × {threads_per_worker} threads each\")\n",
    "    print(f\"✅ Estimated concurrent chunks per worker: {concurrent_chunks_per_worker}\")\n",
    "    print(f\"✅ Max safe chunk size: ~{max_chunk_mb:.2f} MB\")\n",
    "    print(f\"✅ Suggested chunking: time={time_steps_per_year}, lat={suggested_lat}, lon={suggested_lon}\")\n",
    "\n",
    "# Run the function\n",
    "suggest_chunking(client)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8840c675-166e-4c3b-bd59-d7e8d5100493",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-f9ca70f0-7587-4dd8-b6c5-94f7a8f9d3ca</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://127.0.0.1:33345\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 1 \n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 5\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 18.00 GiB\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 0</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:36507\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 5\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/43681/status\" target=\"_blank\">/proxy/43681/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 18.00 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:42725\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/154942544.gadi-pbs/dask-scratch-space/worker-dehkl5kr\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Tasks executing: </strong> \n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Tasks in memory: </strong> \n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Tasks ready: </strong> \n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Tasks in flight: </strong>\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>CPU usage:</strong> 0.0%\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Last seen: </strong> Just now\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory usage: </strong> 88.41 MiB\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Spilled bytes: </strong> 0 B\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Read bytes: </strong> 0.0 B\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Write bytes: </strong> 0.0 B\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>"
      ],
      "text/plain": [
       "{'type': 'Scheduler',\n",
       " 'id': 'Scheduler-f9ca70f0-7587-4dd8-b6c5-94f7a8f9d3ca',\n",
       " 'address': 'tcp://127.0.0.1:33345',\n",
       " 'services': {'dashboard': 8787},\n",
       " 'started': 1763686474.868994,\n",
       " 'n_workers': 1,\n",
       " 'total_threads': 5,\n",
       " 'total_memory': 19327352832,\n",
       " 'workers': {'tcp://127.0.0.1:36507': {'type': 'Worker',\n",
       "   'id': 0,\n",
       "   'host': '127.0.0.1',\n",
       "   'resources': {},\n",
       "   'local_directory': '/jobfs/154942544.gadi-pbs/dask-scratch-space/worker-dehkl5kr',\n",
       "   'name': 0,\n",
       "   'nthreads': 5,\n",
       "   'memory_limit': 19327352832,\n",
       "   'last_seen': 1763686476.410315,\n",
       "   'services': {'dashboard': 43681},\n",
       "   'metrics': {'task_counts': {},\n",
       "    'bandwidth': {'total': 100000000, 'workers': {}, 'types': {}},\n",
       "    'digests_total_since_heartbeat': {},\n",
       "    'managed_bytes': 0,\n",
       "    'spilled_bytes': {'memory': 0, 'disk': 0},\n",
       "    'transfer': {'incoming_bytes': 0,\n",
       "     'incoming_count': 0,\n",
       "     'incoming_count_total': 0,\n",
       "     'outgoing_bytes': 0,\n",
       "     'outgoing_count': 0,\n",
       "     'outgoing_count_total': 0},\n",
       "    'event_loop_interval': 0.02,\n",
       "    'cpu': 0.0,\n",
       "    'memory': 92708864,\n",
       "    'time': 1763686476.3563383,\n",
       "    'host_net_io': {'read_bps': 0.0, 'write_bps': 0.0},\n",
       "    'host_disk_io': {'read_bps': 0.0, 'write_bps': 0.0},\n",
       "    'num_fds': 23},\n",
       "   'status': 'running',\n",
       "   'nanny': 'tcp://127.0.0.1:42725'}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.scheduler_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26d2ff2a-e5c9-4014-9f6b-a30822658602",
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_lat_slice = slice(-50,-25)\n",
    "lon_slice = slice(90,195)\n",
    "baseline_time_range = np.arange(1990,2009+1)\n",
    "\n",
    "turbulence_index = \"windspeed\"\n",
    "p95, p99, p999 = (62.835737276077246, 75.76162159729002, 88.1229062187195)\n",
    "\n",
    "resampler_dict = {\"monthly\":\"ME\", \"ann\":\"YE\", \"seasonal\":\"QS-DEC\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70c830b3-29f5-44d6-9254-626eaebab081",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_evaluation = ['evaluation_BARRA-R_r1i1p1f1',\n",
    "                   'evaluation_ERA5_r1i1p1f1',]\n",
    "\n",
    "list_historical = ['historical_ACCESS-CM2_r4i1p1f1', \n",
    "                   'historical_ACCESS-ESM1-5_r6i1p1f1',\n",
    "                   'historical_CESM2_r11i1p1f1', \n",
    "                   'historical_CMCC-ESM2_r1i1p1f1',\n",
    "                   'historical_EC-Earth3_r1i1p1f1',\n",
    "                   'historical_MPI-ESM1-2-HR_r1i1p1f1',\n",
    "                   'historical_NorESM2-MM_r1i1p1f1',\n",
    "                  ]\n",
    "\n",
    "list_ssp126 = [\n",
    "                 'ssp126_ACCESS-CM2_r4i1p1f1', # need to fix this one\n",
    "                 'ssp126_ACCESS-ESM1-5_r6i1p1f1',\n",
    "                 'ssp126_CESM2_r11i1p1f1',\n",
    "                 'ssp126_CMCC-ESM2_r1i1p1f1',\n",
    "                 'ssp126_EC-Earth3_r1i1p1f1',\n",
    "                 'ssp126_MPI-ESM1-2-HR_r1i1p1f1',\n",
    "                 'ssp126_NorESM2-MM_r1i1p1f1',\n",
    "              ]\n",
    "\n",
    "list_ssp370 = ['ssp370_ACCESS-CM2_r4i1p1f1',\n",
    "                 'ssp370_ACCESS-ESM1-5_r6i1p1f1',\n",
    "                 'ssp370_CESM2_r11i1p1f1',\n",
    "                 'ssp370_CMCC-ESM2_r1i1p1f1',\n",
    "                 'ssp370_EC-Earth3_r1i1p1f1',\n",
    "                 'ssp370_MPI-ESM1-2-HR_r1i1p1f1',\n",
    "                 'ssp370_NorESM2-MM_r1i1p1f1',\n",
    "              ]\n",
    "\n",
    "list_ssp585 = ['ssp585_ACCESS-CM2_r4i1p1f1',\n",
    "                 'ssp585_EC-Earth3_r1i1p1f1']\n",
    "\n",
    "list_future = list_ssp126 + list_ssp370 + list_ssp585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eb7ed25-f00c-4ac2-b5cf-97038dcf5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentiles /quantiles from original data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "505ee9fe-7169-4149-9588-fe0aeac4a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5bdca6c8-ba95-4c03-900f-2d0cfe3414a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monthly\n",
      "evaluation_BARRA-R_r1i1p1f1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "__resample_dim__ must not be empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:28\u001b[0m\n",
      "File \u001b[0;32m/g/data/xp65/public/apps/med_conda/envs/analysis3-25.10/lib/python3.11/site-packages/xarray/backends/api.py:1643\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, errors, **kwargs)\u001b[0m\n\u001b[1;32m   1641\u001b[0m closers \u001b[38;5;241m=\u001b[39m [getattr_(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_close\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1643\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parallel:\n\u001b[1;32m   1646\u001b[0m     \u001b[38;5;66;03m# calling compute here will return the datasets/file_objs lists,\u001b[39;00m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;66;03m# the underlying datasets will still be stored as dask arrays\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m     datasets, closers \u001b[38;5;241m=\u001b[39m dask\u001b[38;5;241m.\u001b[39mcompute(datasets, closers)\n",
      "File \u001b[0;32m/g/data/xp65/public/apps/med_conda/envs/analysis3-25.10/lib/python3.11/site-packages/xarray/backends/api.py:1643\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1641\u001b[0m closers \u001b[38;5;241m=\u001b[39m [getattr_(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_close\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1643\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m [\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m   1645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parallel:\n\u001b[1;32m   1646\u001b[0m     \u001b[38;5;66;03m# calling compute here will return the datasets/file_objs lists,\u001b[39;00m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;66;03m# the underlying datasets will still be stored as dask arrays\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m     datasets, closers \u001b[38;5;241m=\u001b[39m dask\u001b[38;5;241m.\u001b[39mcompute(datasets, closers)\n",
      "File \u001b[0;32m<timed exec>:13\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(ds)\u001b[0m\n",
      "File \u001b[0;32m/g/data/xp65/public/apps/med_conda/envs/analysis3-25.10/lib/python3.11/site-packages/xarray/util/deprecation_helpers.py:119\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._decorator.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(zip_args)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs[:\u001b[38;5;241m-\u001b[39mn_extra_args], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/g/data/xp65/public/apps/med_conda/envs/analysis3-25.10/lib/python3.11/site-packages/xarray/core/dataset.py:10416\u001b[0m, in \u001b[0;36mDataset.resample\u001b[0;34m(self, indexer, skipna, closed, label, offset, origin, restore_coord_dims, **indexer_kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a Resample object for performing resampling operations.\u001b[39;00m\n\u001b[1;32m  10361\u001b[0m \n\u001b[1;32m  10362\u001b[0m \u001b[38;5;124;03mHandles both downsampling and upsampling. The resampled\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10412\u001b[0m \u001b[38;5;124;03m.. [1] https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases\u001b[39;00m\n\u001b[1;32m  10413\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m  10414\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresample\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatasetResample\n\u001b[0;32m> 10416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  10417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresample_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDatasetResample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclosed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10422\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10423\u001b[0m \u001b[43m    \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrestore_coord_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrestore_coord_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10425\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mindexer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10426\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/g/data/xp65/public/apps/med_conda/envs/analysis3-25.10/lib/python3.11/site-packages/xarray/core/common.py:1121\u001b[0m, in \u001b[0;36mDataWithCoords._resample\u001b[0;34m(self, resample_cls, indexer, skipna, closed, label, offset, origin, restore_coord_dims, **indexer_kwargs)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfreq must be an object of type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime.timedelta\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpandas.Timedelta\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpandas.DateOffset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimeResampler\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(freq)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1119\u001b[0m     )\n\u001b[0;32m-> 1121\u001b[0m rgrouper \u001b[38;5;241m=\u001b[39m \u001b[43mResolvedGrouper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrouper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resample_cls(\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1125\u001b[0m     (rgrouper,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     restore_coord_dims\u001b[38;5;241m=\u001b[39mrestore_coord_dims,\n\u001b[1;32m   1129\u001b[0m )\n",
      "File \u001b[0;32m<string>:7\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, grouper, group, obj, eagerly_compute_group)\u001b[0m\n",
      "File \u001b[0;32m/g/data/xp65/public/apps/med_conda/envs/analysis3-25.10/lib/python3.11/site-packages/xarray/core/groupby.py:331\u001b[0m, in \u001b[0;36mResolvedGrouper.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BinGrouper, UniqueGrouper\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouper \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouper)\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m \u001b[43m_resolve_group\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meagerly_compute_group:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEagerly computing the DataArray you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre grouping by (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124m        has been removed.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124m        `.groupby(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=BinGrouper(bins=...))`; as appropriate.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    342\u001b[0m     )\n",
      "File \u001b[0;32m/g/data/xp65/public/apps/med_conda/envs/analysis3-25.10/lib/python3.11/site-packages/xarray/core/groupby.py:505\u001b[0m, in \u001b[0;36m_resolve_group\u001b[0;34m(obj, group)\u001b[0m\n\u001b[1;32m    502\u001b[0m         newgroup \u001b[38;5;241m=\u001b[39m group_da\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newgroup\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnewgroup\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m newgroup\n",
      "\u001b[0;31mValueError\u001b[0m: __resample_dim__ must not be empty"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# annual average distribution\n",
    "turbulence_index = \"windspeed\"\n",
    "for frequency in [\"monthly\"]:#, \"ann\", \"seasonal\"]:\n",
    "    print(frequency)\n",
    "    filename = f\"/scratch/v46/gt3409/turbulence_AUSCAT/{turbulence_index}-{frequency}-percentiles_AUS-15_BOM_BARPA-R_v1-r1_6hr.nc\"\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"File '{filename}' already exists.\")\n",
    "    else:\n",
    "        scaling = {\"ann\":1, \"monthly\":4, \"seasonal\":4}[frequency]\n",
    "        lat_chunksize = int(np.ceil(436/(scaling)))    \n",
    "        def _preprocess(ds):\n",
    "            # rechunk such that there are as many chunks as there are years, \n",
    "            return ds.dropna(\"time\").astype(\"float16\").resample(time=resampler_dict[frequency]).quantile(np.arange(0,1,0.01), dim=\"time\")\n",
    "        \n",
    "        for run in list_evaluation + list_historical:\n",
    "            experiment_id, source_id, member_id = run.split(\"_\")\n",
    "            print(run)\n",
    "            \n",
    "            run_filename = f\"/scratch/v46/gt3409/TMP_{turbulence_index}/TMP_{turbulence_index}-{frequency}-percentiles_AUS-15_{run}_BOM_BARPA-R_v1-r1_6hr.nc\"\n",
    "            if os.path.exists(run_filename):\n",
    "                print(f\"File '{run_filename}' already exists.\")\n",
    "            else:\n",
    "                # wildcard for year\n",
    "                filelist = [f\"/scratch/v46/gt3409/TMP_{turbulence_index}/TMP_{turbulence_index}_AUS-15_{source_id}_{experiment_id}_{member_id}_BOM_BARPA-R_v1-r1_6hr_{year}.nc\"\n",
    "                            for year in np.arange(1990, 2009+1)]\n",
    "                \n",
    "                ds0=xr.open_dataset(filelist[0])\n",
    "                ds = xr.open_mfdataset(filelist, \n",
    "                                       decode_times=True,\n",
    "                                       combine=\"nested\", \n",
    "                                       concat_dim=\"time\",\n",
    "                                       preprocess=_preprocess,\n",
    "                                       chunks = {\"time\":-1, \n",
    "                                                 \"lat\":lat_chunksize, \n",
    "                                                 \"lon\":-1},\n",
    "                                      )\\\n",
    "                        .assign_coords({\"run\":run})\\\n",
    "                        .convert_calendar(\"standard\")\\\n",
    "                        .where(~np.isnan(ds0.isel(time=0)))\n",
    "        \n",
    "    \n",
    "                try:\n",
    "                    # ds.chunk({\"time\":-1, \"lat\":lat_chunksize, \"lon\":-1})\\\n",
    "                        # .resample(time=resampler_dict[frequency])\\\n",
    "                    ds.to_netcdf(run_filename, compute=True)\n",
    "                    print(f\"Made '{run_filename}'\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in {run}: {e}\")     \n",
    "\n",
    "        # print(f\"Combine... \")\n",
    "        # run_filename_list = [f\"/scratch/v46/gt3409/TMP_{turbulence_index}/TMP_{turbulence_index}-{frequency}-percentiles_AUS-15_{run}_BOM_BARPA-R_v1-r1_6hr.nc\"\n",
    "        #                                 for run in list_evaluation + list_historical]\n",
    "        # ds_results = xr.open_mfdataset(run_filename_list,\n",
    "        #                                concat_dim = \"run\",\n",
    "        #                                combine=\"nested\",\n",
    "        #                                )\n",
    "        # ds_results.to_netcdf(filename)\n",
    "        # print(f\"Made '{filename}'\")\n",
    "    # then delete temp files\n",
    "    \n",
    "    # # break out of frequency loop    \n",
    "    # break\n",
    "        # # Calculate results\n",
    "        # results = []\n",
    "        \n",
    "        # for run in list_evaluation: # + list_historical + list_future:\n",
    "        #     print(run)\n",
    "        #     results.append(delayed_results[run].compute())\n",
    "        \n",
    "        # # then save to netCDF\n",
    "        # ds_results_ann = xr.concat(results, dim=\"run\",)\n",
    "        \n",
    "        # # print(\"Saving to netcdf\")\n",
    "        # # ds_results_ann.to_netcdf(filename)\n",
    "        # # print(f\"Made '{filename}'\")\n",
    "                             \n",
    "        # ds_results_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c75bff-2122-48a9-94ac-67eb725e8dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06abb28f-c819-4ab4-a4bd-cdcd800889b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activity_id                                                    [BARPA-R]\n",
       "institution_id                                                     [BOM]\n",
       "version                                                      [v20231001]\n",
       "variable_id                                                      [ua200]\n",
       "table_id                                                           [6hr]\n",
       "source_id              [ACCESS-CM2, ACCESS-ESM1-5, CESM2, CMCC-ESM2, ...\n",
       "experiment_id           [historical, ssp126, ssp370, ssp585, evaluation]\n",
       "member_id                      [r4i1p1f1, r6i1p1f1, r11i1p1f1, r1i1p1f1]\n",
       "grid_label                                                      [AUS-15]\n",
       "time_range             [196001-196012, 196101-196112, 196201-196212, ...\n",
       "path                   [/g/data/py18/BARPA/output/CMIP6/DD/AUS-15/BOM...\n",
       "derived_variable_id                                                   []\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_name = \"barpa\"\n",
    "col = intake.open_esm_datastore(f\"/g/data/lp01/collections/py3.9_dev/nci-{cat_name}.json\")\n",
    "\n",
    "# Edit this cell\n",
    "var_list = [\"ua200\", \"ua250\", \"ua300\", \"va200\", \"va250\", \"va300\", \"ta200\", \"ta250\", \"ta300\", \"zg200\", \"zg250\", \"zg300\"]\n",
    "table_id = \"6hr\"\n",
    "scenarios = [\"historical\",\"ssp126\", \"ssp370\", \"ssp585\", \"evaluation\"]\n",
    "\n",
    "# change this query to select a subset of the data you are interested in\n",
    "query = dict(variable_id = var_list[0],\n",
    "             table_id = table_id,\n",
    "             experiment_id = scenarios,\n",
    "            )\n",
    "\n",
    "cat = col.search(**query)\n",
    "cat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50e9242a-7bc7-4acf-9245-cd42b2691b2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the list of indices for evaluation, historical and future groups.\n",
    "cat_df_max = cat.df.groupby([\"variable_id\", \"experiment_id\", \"source_id\", \"member_id\"]).max().reset_index()\n",
    "cat_df_max[\"index\"] = [f'{cat_df_max.iloc[i][\"experiment_id\"]}_{cat_df_max.iloc[i][\"source_id\"]}_{cat_df_max.iloc[i][\"member_id\"]}' for i in np.arange(len(cat_df_max))]\n",
    "cat_df_max = cat_df_max.set_index(\"index\")\n",
    "# cat_df_max\n",
    "\n",
    "# indices for evaluation, historical and future groups. These will share time ranges\n",
    "i_evaluation = cat_df_max.loc[cat_df_max[\"experiment_id\"].isin([\"evaluation\"])].index\n",
    "i_historical = cat_df_max.loc[cat_df_max[\"experiment_id\"].isin([\"historical\"])].index\n",
    "i_future = cat_df_max.loc[cat_df_max[\"experiment_id\"].isin([\"ssp126\", \"ssp370\", \"ssp585\"])].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aba7e67-9bbf-4034-a7d9-f07be1edc40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation_ERA5_r1i1p1f1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'baseline_time_range' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'baseline_time_range' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "turbulence_index = \"windspeed\"\n",
    "for run in ['evaluation_ERA5_r1i1p1f1']:\n",
    "    experiment_id, source_id, member_id = run.split(\"_\")\n",
    "    print(run)\n",
    "    time_range = baseline_time_range\n",
    "    \n",
    "    filelist = [f\"/scratch/v46/gt3409/TMP_{turbulence_index}/TMP_{turbulence_index}_AUS-15_{source_id}_{experiment_id}_{member_id}_BOM_BARPA-R_v1-r1_6hr_{year}.nc\" \n",
    "     for year in time_range]\n",
    "    \n",
    "    def _preprocess(ds, q=[0.95, 0.99, 0.999],):\n",
    "        return ds.sel(lat=mid_lat_slice, lon=lon_slice).chunk({\"time\":-1, \"lat\":-1, \"lon\":-1}).quantile(q, dim=[\"time\", \"lat\", \"lon\"])\n",
    "\n",
    "    ds = xr.open_mfdataset(filelist, use_cftime=True, preprocess=_preprocess, combine=\"nested\", concat_dim=\"time\")\n",
    "    ds = ds.compute()\n",
    "    \n",
    "p95, p99, p999 = ds.mean(\"time\")[\"windspeed\"].values\n",
    "p95, p99, p999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26180044-d736-4124-82e7-92d76f990734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation_BARRA-R_r1i1p1f1\n",
      "CPU times: user 4.84 s, sys: 2.1 s, total: 6.94 s\n",
      "Wall time: 1min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(62.835737276077246, 75.76162159729002, 88.1229062187195)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Determine threshold MOG from evaluation dataset\n",
    "turbulence_index = \"windspeed\"\n",
    "for run in ['evaluation_BARRA-R_r1i1p1f1']:\n",
    "    experiment_id, source_id, member_id = run.split(\"_\")\n",
    "    print(run)\n",
    "    time_range = baseline_time_range\n",
    "    \n",
    "    filelist = [f\"/scratch/v46/gt3409/TMP_{turbulence_index}/TMP_{turbulence_index}_AUS-15_{source_id}_{experiment_id}_{member_id}_BOM_BARPA-R_v1-r1_6hr_{year}.nc\" \n",
    "     for year in time_range]\n",
    "    \n",
    "    def _preprocess(ds, q=[0.95, 0.99, 0.999],):\n",
    "        return ds.sel(lat=mid_lat_slice, lon=lon_slice).chunk({\"time\":-1, \"lat\":-1, \"lon\":-1}).quantile(q, dim=[\"time\", \"lat\", \"lon\"])\n",
    "\n",
    "    ds = xr.open_mfdataset(filelist, decode_times=True, preprocess=_preprocess, combine=\"nested\", concat_dim=\"time\")\n",
    "    ds = ds.compute()\n",
    "p95, p99, p999 = ds.mean(\"time\")[\"windspeed\"].values\n",
    "p95, p99, p999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d33d693-3b00-4df2-b85b-a24754cec59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate frequency above thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9b927-1bcb-437b-a0ee-687a0de03c28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing ssp126_ACCESS-CM2_r4i1p1f1...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# annual monthly value mapped\n",
    "# using XXL 28 cores client = Client(threads_per_worker=4, n_workers=2)\n",
    "\n",
    "frequency = \"monthly\"\n",
    "for frequency in [\"monthly\"]:# , \"ann\"]:    \n",
    "    filename = f\"/scratch/v46/gt3409/turbulence_AUSCAT/{turbulence_index}-{frequency}-freq-above-p99_mapped_new.nc\"\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"File '{filename}' already exists.\")\n",
    "    else:\n",
    "        def _preprocess(ds):    \n",
    "            \"\"\"Calculate frequency of exceeding p99 threshold\"\"\"\n",
    "            # ds = ds.chunk({\"time\": -1, \"lat\": 10, \"lon\": 40})\n",
    "            return (ds.dropna(\"time\") > p99).astype(\"float32\").where(~np.isnan(ds)).resample(time=resampler_dict[frequency]).mean(\"time\")\n",
    "            \n",
    "        computed_files = []\n",
    "        for run in list_evaluation + list_historical + list_future:\n",
    "            file_to_compute = f\"/scratch/v46/gt3409/TMP_{turbulence_index}-freq-above-p99/TMP_{turbulence_index}-{frequency}-freq-above-p99_{run}.nc\"\n",
    "            \n",
    "            if os.path.exists(file_to_compute):\n",
    "                continue\n",
    "                print(f\"File '{file_to_compute}' already exists.\")\n",
    "            else:\n",
    "                experiment_id, source_id, member_id = run.split(\"_\")\n",
    "                print(f\"Computing {run}...\")\n",
    "                \n",
    "                # wildcard for year\n",
    "                filelist = sorted(glob.glob(f\"/scratch/v46/gt3409/TMP_{turbulence_index}/TMP_{turbulence_index}_AUS-15_{source_id}_{experiment_id}_{member_id}_BOM_BARPA-R_v1-r1_6hr_*.nc\"))\n",
    "                \n",
    "                delayed_results = xr.open_mfdataset(filelist, decode_times=False,\n",
    "                                                       preprocess=_preprocess, combine=\"nested\", \n",
    "                                                       concat_dim=\"time\", \n",
    "                                                   chunks={\"time\": -1, \"lat\": -1, \"lon\": -1})\\\n",
    "                                        .assign_coords({\"run\":run})\\\n",
    "                                        .convert_calendar(\"standard\")\n",
    "                try:\n",
    "                    delayed_results.compute().to_netcdf(file_to_compute)\n",
    "                    print(f\"file saved {file_to_compute}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in {run}: {e}\")\n",
    "\n",
    "    \n",
    "            computed_files.append(file_to_compute)\n",
    "    \n",
    "        # open all made files, combine and compute\n",
    "        ds = xr.open_mfdataset(computed_files,\n",
    "                           combine=\"nested\",\n",
    "                           concat_dim=\"run\",\n",
    "                           ).compute()\n",
    "        ds.to_netcdf(filename)\n",
    "        # select mid lat box and compute frequency\n",
    "        ds.sel(lat=mid_lat_slice, lon=lon_slice)\\\n",
    "          .mean([\"lat\", \"lon\"])\\\n",
    "          .to_netcdf(f\"/scratch/v46/gt3409/turbulence_AUSCAT/{turbulence_index}-{frequency}-freq-above-p99_midlatbox.nc\")\n",
    "        print(f\"Made {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe389d8-c13e-4345-aca3-06ec31a4d0de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# use client = Client(threads_per_worker=1, n_workers=4, timeout=\"600s\") with 28cores\n",
    "frequency = \"ann\"\n",
    "\n",
    "# annual average value mapped\n",
    "filename = f\"/scratch/v46/gt3409/turbulence_AUSCAT/{turbulence_index}-{frequency}-freq-above-p99_mapped.nc\"\n",
    "if os.path.exists(filename):\n",
    "    print(f\"File '{filename}' already exists.\")\n",
    "else:\n",
    "    def _preprocess(ds):    \n",
    "        \"\"\"Calculate annual frequency of exceeding p99 threshold\"\"\"\n",
    "        ds = ds.chunk({\"time\": -1, \"lat\": 10, \"lon\": 40})\n",
    "        new_ds = (ds > p99).astype(\"float32\").where(~np.isnan(ds))\n",
    "        new_ds = new_ds.resample(time=resampler_dict[frequency]).mean(\"time\")\n",
    "        return new_ds\n",
    "        \n",
    "    for run in list_evaluation + list_historical + list_future:\n",
    "        file_to_compute = f\"/scratch/v46/gt3409/TMP_{turbulence_index}-freq-above-p99/TMP_{turbulence_index}-freq-above-p99_{run}.nc\"\n",
    "        # file_to_compute = f\"/scratch/v46/gt3409/TMP_{turbulence_index}-freq-above-p99/TMP_{turbulence_index}-{frequency}-freq-above-p99_{run}.nc\"\n",
    "        if os.path.exists(file_to_compute):\n",
    "            continue\n",
    "            print(f\"File '{file_to_compute}' already exists.\")\n",
    "        else:\n",
    "            experiment_id, source_id, member_id = run.split(\"_\")\n",
    "            print(f\"Computing {run}...\")\n",
    "            \n",
    "            # wildcard for year\n",
    "            filelist = sorted(glob.glob(f\"/scratch/v46/gt3409/TMP_{turbulence_index}/TMP_{turbulence_index}_AUS-15_{source_id}_{experiment_id}_{member_id}_BOM_BARPA-R_v1-r1_6hr_*.nc\"))\n",
    "            \n",
    "            delayed_results = xr.open_mfdataset(filelist, use_cftime=False,\n",
    "                                                   preprocess=_preprocess, combine=\"nested\", \n",
    "                                                   concat_dim=\"time\", )\\\n",
    "                                    .assign_coords({\"run\":run})\\\n",
    "                                    .convert_calendar(\"standard\")\n",
    "\n",
    "            delayed_results.compute().to_netcdf(file_to_compute)\n",
    "            print(f\"file saved {file_to_compute}\")\n",
    "\n",
    "    # open all made files, combine and compute\n",
    "    # ds = xr.open_mfdataset([f\"/scratch/v46/gt3409/TMP_{turbulence_index}-freq-above-p99/TMP_{turbulence_index}-{frequency}-freq-above-p99_{run}.nc\"\n",
    "    ds = xr.open_mfdataset([f\"/scratch/v46/gt3409/TMP_{turbulence_index}-freq-above-p99/TMP_{turbulence_index}-freq-above-p99_{run}.nc\"\n",
    "                        for run in list_evaluation + list_historical + list_future],\n",
    "                       combine=\"nested\",\n",
    "                       concat_dim=\"run\",\n",
    "                       ).compute()\n",
    "    ds.to_netcdf(filename)\n",
    "    # select mid lat box and compute frequency\n",
    "    ds.sel(lat=mid_lat_slice, lon=lon_slice)\\\n",
    "      .mean([\"lat\", \"lon\"])\\\n",
    "      .to_netcdf(f\"/scratch/v46/gt3409/turbulence_AUSCAT/{turbulence_index}-{frequency}-freq-above-p99_midlatbox.nc\")\n",
    "    print(f\"Made {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06977e4-5815-4f62-8af3-9928393e8aa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing ssp370_MPI-ESM1-2-HR_r1i1p1f1...\n",
      "file saved /scratch/v46/gt3409/TMP_windspeed-freq-above-p99/TMP_windspeed-seasonal-freq-above-p99_ssp370_MPI-ESM1-2-HR_r1i1p1f1.nc\n",
      "Computing ssp370_NorESM2-MM_r1i1p1f1...\n",
      "file saved /scratch/v46/gt3409/TMP_windspeed-freq-above-p99/TMP_windspeed-seasonal-freq-above-p99_ssp370_NorESM2-MM_r1i1p1f1.nc\n",
      "Computing ssp585_ACCESS-CM2_r4i1p1f1...\n",
      "file saved /scratch/v46/gt3409/TMP_windspeed-freq-above-p99/TMP_windspeed-seasonal-freq-above-p99_ssp585_ACCESS-CM2_r4i1p1f1.nc\n",
      "Computing ssp585_EC-Earth3_r1i1p1f1...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#seasonal must be different due to  seasonal resampling after open mf\n",
    "\n",
    "resampler_dict = {\"monthly\":\"ME\", \"ann\":\"YE\", \"seasonal\":\"QS-DEC\"}\n",
    "frequency = \"seasonal\"\n",
    "  \n",
    "filename = f\"/scratch/v46/gt3409/turbulence_AUSCAT/{turbulence_index}-{frequency}-freq-above-p99_mapped.nc\"\n",
    "if os.path.exists(filename):\n",
    "    print(f\"File '{filename}' already exists.\")\n",
    "else:\n",
    "    def _preprocess(ds):    \n",
    "        \"\"\"Calculate frequency of exceeding p99 threshold\"\"\"\n",
    "        return (ds > p99).astype(\"float32\").where(~np.isnan(ds))\n",
    "        \n",
    "    computed_files = []\n",
    "    for run in list_evaluation + list_historical + list_future:\n",
    "        file_to_compute = f\"/scratch/v46/gt3409/TMP_{turbulence_index}-freq-above-p99/TMP_{turbulence_index}-{frequency}-freq-above-p99_{run}.nc\"\n",
    "        \n",
    "        if os.path.exists(file_to_compute):\n",
    "            continue\n",
    "            print(f\"File '{file_to_compute}' already exists.\")\n",
    "        else:\n",
    "            experiment_id, source_id, member_id = run.split(\"_\")\n",
    "            print(f\"Computing {run}...\")\n",
    "            \n",
    "            # wildcard for year\n",
    "            filelist = sorted(glob.glob(f\"/scratch/v46/gt3409/TMP_{turbulence_index}/TMP_{turbulence_index}_AUS-15_{source_id}_{experiment_id}_{member_id}_BOM_BARPA-R_v1-r1_6hr_*.nc\"))\n",
    "            \n",
    "            delayed_results = xr.open_mfdataset(filelist, use_cftime=False,\n",
    "                                                   preprocess=_preprocess, combine=\"nested\", \n",
    "                                                   concat_dim=\"time\", \n",
    "                                                    chunks={\"time\": -1, \"lat\": -1, \"lon\": -1})\\\n",
    "                                    .chunk({\"time\": -1,\"lat\": 4, \"lon\": -1})\\\n",
    "                                    .resample(time=resampler_dict[frequency]).mean(\"time\")\\\n",
    "                                    .assign_coords({\"run\":run})\\\n",
    "                                    .convert_calendar(\"standard\")\n",
    "            try:\n",
    "                delayed_results.compute().to_netcdf(file_to_compute)\n",
    "                print(f\"file saved {file_to_compute}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {run}: {e}\")\n",
    "\n",
    "        computed_files.append(file_to_compute)\n",
    "\n",
    "    # open all made files, combine and compute\n",
    "    ds = xr.open_mfdataset(computed_files,\n",
    "                       combine=\"nested\",\n",
    "                       concat_dim=\"run\",\n",
    "                       ).compute()\n",
    "    ds.to_netcdf(filename)\n",
    "    # select mid lat box and compute frequency\n",
    "    ds.sel(lat=mid_lat_slice, lon=lon_slice)\\\n",
    "      .mean([\"lat\", \"lon\"])\\\n",
    "      .to_netcdf(f\"/scratch/v46/gt3409/turbulence_AUSCAT/{turbulence_index}-{frequency}-freq-above-p99_midlatbox.nc\")\n",
    "    print(f\"Made {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cdc396-1e29-4c48-b092-8af560be5396",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# annual average value for mid latitudes\n",
    "filename = f\"/scratch/v46/gt3409/turbulence_AUSCAT/{turbulence_index}-freq-above-p99_AUS-15_BOM_BARPA-R_v1-r1_6hr_ann.nc\"\n",
    "if os.path.exists(filename):\n",
    "    print(f\"File '{filename}' already exists.\")\n",
    "else:\n",
    "    delayed_results = {}\n",
    "    \n",
    "    turbulence_index = \"windspeed\"\n",
    "    for run in list_evaluation + list_historical + list_future:\n",
    "        experiment_id, source_id, member_id = run.split(\"_\")\n",
    "        print(run)\n",
    "        \n",
    "        # wildcard for year\n",
    "        filelist = sorted(glob.glob(f\"/scratch/v46/gt3409/TMP_{turbulence_index}/TMP_{turbulence_index}_AUS-15_{source_id}_{experiment_id}_{member_id}_BOM_BARPA-R_v1-r1_6hr_*.nc\"))\n",
    "        \n",
    "        def _preprocess(ds):\n",
    "            \"\"\"Calculate annual frequency of exceeding p99 threshold\"\"\"\n",
    "            ds = (ds.sel(lat=mid_lat_slice, lon=lon_slice)>p99)\\\n",
    "                    .mean([\"time\", \"lat\", \"lon\"])\\\n",
    "                    .assign_coords({\"time\":ds.isel({\"time\":0})[\"time\"].dt.year})\n",
    "            \n",
    "            # ds = (ds.sel(lat=mid_lat_slice, lon=lon_slice)>p99).convert_calendar(\"standard\")\\\n",
    "            #         .mean([\"lat\", \"lon\"]).resample({\"time\":\"YE\"},).mean([\"time\"])\n",
    "            return ds\n",
    "        \n",
    "        ds = xr.open_mfdataset(filelist, use_cftime=True,\n",
    "                               preprocess=_preprocess, combine=\"nested\", \n",
    "                               concat_dim=\"time\").assign_coords({\"run\":run})\n",
    "    \n",
    "        delayed_results[run] = ds\n",
    "    \n",
    "    \n",
    "    # Calculate results\n",
    "    results = []\n",
    "    \n",
    "    for run in list_evaluation + list_historical + list_future:\n",
    "        print(run)\n",
    "        results.append(delayed_results[run].compute())\n",
    "    \n",
    "    # then save to netCDF\n",
    "    ds_results = xr.concat(results, dim=\"run\",)\n",
    "    \n",
    "    print(\"Saving to netcdf\")\n",
    "    ds_results.to_netcdf(filename)\n",
    "    print(f\"Made '{filename}'\")\n",
    "                         \n",
    "    ds_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dae879b-6e7a-49cd-9a41-d1ddcc970cca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# monthly average value for mid latitudes\n",
    "filename = f\"/scratch/v46/gt3409/turbulence_AUSCAT/{turbulence_index}-freq-above-p99_AUS-15_BOM_BARPA-R_v1-r1_6hr_monthly.nc\"\n",
    "if os.path.exists(filename):\n",
    "    print(f\"File '{filename}' already exists.\")\n",
    "else:\n",
    "    delayed_results = {}\n",
    "    \n",
    "    turbulence_index = \"windspeed\"\n",
    "    for run in list_evaluation + list_historical + list_future:\n",
    "        experiment_id, source_id, member_id = run.split(\"_\")\n",
    "        print(run)\n",
    "        \n",
    "        # wildcard for year\n",
    "        filelist = sorted(glob.glob(f\"/scratch/v46/gt3409/TMP_{turbulence_index}/TMP_{turbulence_index}_AUS-15_{source_id}_{experiment_id}_{member_id}_BOM_BARPA-R_v1-r1_6hr_*.nc\"))\n",
    "        \n",
    "        def _preprocess(ds):\n",
    "            \"\"\"Calculate annual frequency of exceeding p99 threshold\"\"\"\n",
    "            ds = (ds.sel(lat=mid_lat_slice, lon=lon_slice)>p99)\\\n",
    "                    .convert_calendar(\"standard\")\\\n",
    "                    .mean([\"lat\", \"lon\"]).resample({\"time\":\"ME\"},).mean([\"time\"])\n",
    "            return ds\n",
    "        \n",
    "        ds = xr.open_mfdataset(filelist, use_cftime=True,\n",
    "                               preprocess=_preprocess, combine=\"nested\", \n",
    "                               concat_dim=\"time\").assign_coords({\"run\":run})\n",
    "    \n",
    "        delayed_results[run] = ds\n",
    "    \n",
    "    \n",
    "    # Calculate results\n",
    "    results = []\n",
    "    \n",
    "    for run in list_evaluation + list_historical + list_future:\n",
    "        print(run)\n",
    "        results.append(delayed_results[run].compute())\n",
    "    \n",
    "    # then save to netCDF\n",
    "    ds_results = xr.concat(results, dim=\"run\",)\n",
    "    \n",
    "    print(\"Saving to netcdf\")\n",
    "    ds_results.to_netcdf(filename)\n",
    "    print(f\"Made '{filename}'\")\n",
    "                         \n",
    "    ds_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597e209-6f7e-43a9-a89c-f810ce807a52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# seasonal average value for mid latitudes\n",
    "filename = f\"/scratch/v46/gt3409/turbulence_AUSCAT/{turbulence_index}-freq-above-p99_AUS-15_BOM_BARPA-R_v1-r1_6hr_seasonal.nc\"\n",
    "if os.path.exists(filename):\n",
    "    print(f\"File '{filename}' already exists.\")\n",
    "else:\n",
    "    #took about 90 min with 28 cores, 28 threads and 7 workers\n",
    "    delayed_results = {}\n",
    "    \n",
    "    turbulence_index = \"windspeed\"\n",
    "    for run in list_evaluation + list_historical + list_future:\n",
    "        experiment_id, source_id, member_id = run.split(\"_\")\n",
    "        print(run)\n",
    "        \n",
    "        # wildcard for year\n",
    "        filelist = sorted(glob.glob(f\"/scratch/v46/gt3409/TMP_{turbulence_index}/TMP_{turbulence_index}_AUS-15_{source_id}_{experiment_id}_{member_id}_BOM_BARPA-R_v1-r1_6hr_*.nc\"))\n",
    "        \n",
    "        def _preprocess(ds):\n",
    "            \"\"\"Calculate annual frequency of exceeding p99 threshold\"\"\"\n",
    "            ds = (ds.sel(lat=mid_lat_slice, lon=lon_slice)>p99)\\\n",
    "                    .convert_calendar(\"standard\")\\\n",
    "                    .mean([\"lat\", \"lon\"])\n",
    "            return ds\n",
    "        \n",
    "        ds = xr.open_mfdataset(filelist, use_cftime=True,\n",
    "                               preprocess=_preprocess, combine=\"nested\", \n",
    "                               concat_dim=\"time\")\n",
    "        \n",
    "        ds = ds.resample({\"time\":\"QS-DEC\"},).mean([\"time\"]).assign_coords({\"run\":run})\n",
    "    \n",
    "        delayed_results[run] = ds\n",
    "    \n",
    "    \n",
    "    # Calculate results\n",
    "    results = []\n",
    "    \n",
    "    for run in list_evaluation + list_historical + list_future:\n",
    "        print(run)\n",
    "        results.append(delayed_results[run].compute())\n",
    "    \n",
    "    # then save to netCDF\n",
    "    ds_results = xr.concat(results, dim=\"run\",)\n",
    "    \n",
    "    print(\"Saving to netcdf\")\n",
    "    ds_results.to_netcdf(filename)\n",
    "    print(f\"Made '{filename}'\")\n",
    "                         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86db8b7-9109-4791-81b9-17742f31bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f4a78-4046-4675-87ae-7ddd4801cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = \"evaluation_BARRA-R_r1i1p1f1\"\n",
    "\n",
    "# experiment_id, source_id, member_id = run.split(\"_\")\n",
    "# ds = xr.open_dataset(f\"/scratch/v46/gt3409/TMP_{turbulence_index}/TMP_{turbulence_index}_AUS-15_{source_id}_{experiment_id}_{member_id}_BOM_BARPA-R_v1-r1_6hr_2000.nc\")\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e016baee-25e2-4f24-adc5-5a3b1a5eed15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# monthly average value mapped\n",
    "filename = f\"/scratch/v46/gt3409/turbulence_AUSCAT/{turbulence_index}-freq-above-p99_AUS-15_BOM_BARPA-R_v1-r1_6hr_mon-mapped.nc\"\n",
    "if os.path.exists(filename):\n",
    "    print(f\"File '{filename}' already exists.\")\n",
    "else:\n",
    "    delayed_results = {}\n",
    "    \n",
    "    for run in list_evaluation + list_historical + list_future:\n",
    "        experiment_id, source_id, member_id = run.split(\"_\")\n",
    "        print(run)\n",
    "        \n",
    "        # wildcard for year\n",
    "        filelist = sorted(glob.glob(f\"/scratch/v46/gt3409/TMP_{turbulence_index}/TMP_{turbulence_index}_AUS-15_{source_id}_{experiment_id}_{member_id}_BOM_BARPA-R_v1-r1_6hr_*.nc\"))\n",
    "        \n",
    "        def _preprocess(ds):\n",
    "            \"\"\"Calculate annual frequency of exceeding p99 threshold\"\"\"\n",
    "            ds = (ds.chunk({\"time\":-1, \"lat\":100, \"lon\":80})>p99)\\\n",
    "                        .where(np.isnan(ds)==False)\\\n",
    "                        .convert_calendar(\"standard\")\\\n",
    "                        .resample({\"time\":\"ME\"},).mean([\"time\"])\n",
    "            return ds\n",
    "        \n",
    "        ds = xr.open_mfdataset(filelist, use_cftime=True,\n",
    "                               preprocess=_preprocess, combine=\"nested\", \n",
    "                               concat_dim=\"time\").assign_coords({\"run\":run})\n",
    "    \n",
    "        delayed_results[run] = ds\n",
    "    \n",
    "    \n",
    "    # Calculate results\n",
    "    results = []\n",
    "    \n",
    "    for run in list_evaluation + list_historical + list_future:\n",
    "        print(run)\n",
    "        results.append(delayed_results[run].compute())\n",
    "    \n",
    "    # then save to netCDF\n",
    "    ds_results_mon = xr.concat(results, dim=\"run\",)\n",
    "    \n",
    "    print(\"Saving to netcdf\")\n",
    "    ds_results_mon.to_netcdf(filename)\n",
    "    print(f\"Made '{filename}'\")\n",
    "                         \n",
    "    ds_results_mon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a04001d-6e9b-4474-a38f-334dabbfa470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# seasonal average values mapped\n",
    "filename = f\"/scratch/v46/gt3409/turbulence_AUSCAT/{turbulence_index}-freq-above-p99_AUS-15_BOM_BARPA-R_v1-r1_6hr_seasonal-mapped.nc\"\n",
    "if os.path.exists(filename):\n",
    "    print(f\"File '{filename}' already exists.\")\n",
    "    ds_results_seas = xr.open_dataset(filename)\n",
    "else:\n",
    "    delayed_results = {}\n",
    "    \n",
    "    turbulence_index = \"windspeed\"\n",
    "    for run in list_evaluation + list_historical + list_future:\n",
    "        experiment_id, source_id, member_id = run.split(\"_\")\n",
    "        print(run)\n",
    "        \n",
    "        # wildcard for year\n",
    "        filelist = sorted(glob.glob(f\"/scratch/v46/gt3409/TMP_{turbulence_index}/TMP_{turbulence_index}_AUS-15_{source_id}_{experiment_id}_{member_id}_BOM_BARPA-R_v1-r1_6hr_*.nc\"))\n",
    "        \n",
    "        def _preprocess(ds):\n",
    "            \"\"\"Calculate annual frequency of exceeding p99 threshold\"\"\"\n",
    "            ds = (ds.chunk({\"time\":-1, \"lat\":100, \"lon\":80})>p99)\\\n",
    "                    .where(np.isnan(ds)==False)\\\n",
    "                    .convert_calendar(\"standard\")\n",
    "            return ds\n",
    "        \n",
    "        ds = xr.open_mfdataset(filelist, use_cftime=True,\n",
    "                               preprocess=_preprocess, combine=\"nested\", \n",
    "                               concat_dim=\"time\")\n",
    "        \n",
    "        ds = ds.resample({\"time\":\"QS-DEC\"},).mean([\"time\"]).assign_coords({\"run\":run})\n",
    "    \n",
    "        delayed_results[run] = ds\n",
    "    \n",
    "    \n",
    "    # Calculate results\n",
    "    results = []\n",
    "    \n",
    "    for run in list_evaluation + list_historical + list_future:\n",
    "        print(run)\n",
    "        results.append(delayed_results[run].compute())\n",
    "    \n",
    "    # then save to netCDF\n",
    "    ds_results_seas = xr.concat(results, dim=\"run\",)\n",
    "    \n",
    "    print(\"Saving to netcdf\")\n",
    "    ds_results.to_netcdf(filename)\n",
    "    print(f\"Made '{filename}'\")\n",
    "                         \n",
    "ds_results_seas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995cec0-178a-4065-bb01-994d4d50894c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c320348-f8d8-4619-bd06-32ae96d0eeb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3]",
   "language": "python",
   "name": "conda-env-analysis3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
